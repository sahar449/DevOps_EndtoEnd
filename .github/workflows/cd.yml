name: CD to EKS

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types: [completed]
  workflow_dispatch:
    inputs:
      action:
        description: "Deploy or Destroy"
        required: true
        default: "deploy"
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc
  ARGOCD_CLB: eksdemo-argocd-clb
  FLASK_ALB: eksdemo-flask-alb

jobs:
  deploy:
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      # ========== DEPLOY ==========
      - name: Setup kubectl
        if: github.event.inputs.action != 'destroy'
        id: kubectl
        continue-on-error: true
        run: |
          if ! aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} &>/dev/null; then
            echo "‚ö†Ô∏è Cluster not ready yet, skipping deployment"
            exit 1
          fi
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
          kubectl get nodes

      - name: Configure aws-auth
        if: steps.kubectl.outcome == 'success' && github.event.inputs.action != 'destroy'
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF

      - name: Install ArgoCD
        if: steps.kubectl.outcome == 'success' && github.event.inputs.action != 'destroy'
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=classic
          
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m

      - name: Get VPC and Deploy Apps
        if: steps.kubectl.outcome == 'success' && github.event.inputs.action != 'destroy'
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          
          envsubst < ArgoCD/externalCharts/infra.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          
          sleep 10
          kubectl get applications -n argocd

      - name: Smoke Test
        if: steps.kubectl.outcome == 'success' && github.event.inputs.action != 'destroy'
        run: |
          echo "Testing https://www.saharbittman.com"
          for i in {1..30}; do
            status=$(curl -s -o /dev/null -w "%{http_code}" https://www.saharbittman.com || echo "000")
            if [ "$status" = "200" ]; then
              echo "‚úÖ App is live!"
              exit 0
            fi
            echo "‚è≥ Attempt $i/30 - Status: $status"
            sleep 15
          done
          echo "‚ùå Smoke test failed"
          exit 1

      # ========== DESTROY ==========
      - name: Destroy Everything
        if: (github.event.inputs.action == 'destroy' || failure()) && steps.kubectl.outcome == 'success'
        run: |
          echo "üóëÔ∏è Starting cleanup..."
          
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }} || true

          # Remove Kubernetes finalizers
          kubectl get applications.argoproj.io -n argocd -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl get ingress -A -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -p '{"metadata":{"finalizers":[]}}' --type=merge || true

          # Delete Kubernetes resources
          echo "Deleting ingresses first..."
          kubectl delete ingress --all -A --timeout=60s || true
          
          echo "Deleting applications..."
          kubectl delete applications.argoproj.io --all -n argocd --timeout=60s || true
          
          echo "Uninstalling ArgoCD..."
          helm uninstall argocd -n argocd --timeout=2m || true

          echo "‚è≥ Waiting for AWS to process deletions..."
          sleep 30

          # Delete Flask ALB and Target Groups
          echo "üóëÔ∏è Processing Flask ALB..."
          alb=$(aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} \
            --names ${{ env.FLASK_ALB }} \
            --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
          
          if [ -n "$alb" ] && [ "$alb" != "None" ]; then
            echo "Found ALB: $alb"
            
            # Step 1: Get all target groups
            echo "Getting Target Groups..."
            tg_list=$(aws elbv2 describe-target-groups \
              --load-balancer-arn "$alb" --region ${{ env.AWS_REGION }} \
              --query 'TargetGroups[].TargetGroupArn' --output text 2>/dev/null || echo "")
            
            # Step 2: Deregister and delete target groups FIRST
            if [ -n "$tg_list" ]; then
              for tg in $tg_list; do
                echo "Processing Target Group: $tg"
                
                # Deregister all targets
                targets=$(aws elbv2 describe-target-health --target-group-arn "$tg" \
                  --region ${{ env.AWS_REGION }} \
                  --query 'TargetHealthDescriptions[].Target.Id' --output text 2>/dev/null || echo "")
                
                for target in $targets; do
                  echo "Deregistering target: $target"
                  aws elbv2 deregister-targets --target-group-arn "$tg" \
                    --targets Id=$target --region ${{ env.AWS_REGION }} 2>/dev/null || true
                done
                
                sleep 3
                
                # Delete target group
                echo "Deleting Target Group: $tg"
                aws elbv2 delete-target-group --target-group-arn "$tg" \
                  --region ${{ env.AWS_REGION }} 2>/dev/null || true
              done
              echo "‚úÖ Target Groups deleted"
            fi
            
            # Step 3: Now delete the ALB
            sleep 5
            echo "Deleting ALB..."
            aws elbv2 delete-load-balancer --load-balancer-arn "$alb" \
              --region ${{ env.AWS_REGION }} || true
            echo "‚úÖ Flask ALB deleted"
          else
            echo "‚ÑπÔ∏è Flask ALB not found"
          fi

          # Cleanup ALL orphaned Target Groups with eksdemo pattern
          echo "üßπ Cleaning orphaned Target Groups..."
          sleep 10
          
          all_tgs=$(aws elbv2 describe-target-groups --region ${{ env.AWS_REGION }} \
            --query "TargetGroups[?contains(TargetGroupName, 'eksdemo')].TargetGroupArn" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$all_tgs" ]; then
            for tg in $all_tgs; do
              echo "Force deleting Target Group: $tg"
              
              # Deregister all targets
              targets=$(aws elbv2 describe-target-health --target-group-arn "$tg" \
                --region ${{ env.AWS_REGION }} \
                --query 'TargetHealthDescriptions[].Target.Id' --output text 2>/dev/null || echo "")
              
              for target in $targets; do
                aws elbv2 deregister-targets --target-group-arn "$tg" \
                  --targets Id=$target --region ${{ env.AWS_REGION }} 2>/dev/null || true
              done
              
              sleep 3
              aws elbv2 delete-target-group --target-group-arn "$tg" \
                --region ${{ env.AWS_REGION }} 2>/dev/null || true
            done
            echo "‚úÖ All Target Groups cleaned"
          else
            echo "‚ÑπÔ∏è No orphaned Target Groups found"
          fi

          echo "‚úÖ Cleanup complete"