name: CD to EKS

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      action:
        description: "Apply or Destroy"
        required: true
        default: "apply"
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: eksdemo-cluster
  VPC_NAME: eksdemo-vpc
  ARGOCD_CLB: eksdemo-argocd-clb
  FLASK_ALB: eksdemo-flask-alb

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      # Update kubeconfig
      - name: Update kubeconfig for EKS
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}

      # Configure aws-auth
      - name: Configure aws-auth
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF

      # Install ArgoCD
      - name: Install ArgoCD
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set-string server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=classic \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.ARGOCD_CLB }}"
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m

      # Deploy apps
      - name: Get VPC and Deploy Apps
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          export VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.VPC_NAME }}" \
            --query "Vpcs[0].VpcId" --output text)
          export ECR_NAME=${{ secrets.ECR_NAME }}
          export AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}
          envsubst < ArgoCD/externalCharts/infra.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          sleep 10
          kubectl get applications -n argocd

      # Smoke test
      - name: Smoke Test
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply' || github.event_name == 'workflow_run' }}
        run: |
          for i in {1..100}; do
            http_status=$(curl -s -o /dev/null -w "%{http_code}" https://www.saharbittman.com || echo "000")

            if [ "$http_status" = "200" ]; then
              echo "âœ… App is live! ($http_status)"
              exit 0
            fi

            echo "â³ Attempt $i/100 - Status: $http_status"
            sleep 15
          done

          echo "âŒ Smoke test failed"
          exit 1


      # ========== DESTROY ==========
      - name: Destroy Everything
        if: github.event.inputs.action == 'destroy' || failure()
        run: |
          echo "ðŸ—‘ï¸ Starting cleanup..."

          # Ensure kubeconfig is updated for cluster access
          aws eks update-kubeconfig --region "${{ env.AWS_REGION }}" --name "${{ env.CLUSTER_NAME }}" || true

          echo "ðŸ”§ Removing Kubernetes finalizers..."
          # 1. Remove ArgoCD ApplicationSet, Application, and Ingress Finalizers.
          # This is critical to allow Kubernetes to fully delete the resources, 
          # and signals the AWS LBC to clean up AWS resources (if it can).
          kubectl get applicationset.argoproj.io -n argocd -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl get applications.argoproj.io -n argocd -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl get ingress -A -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -p '{"metadata":{"finalizers":[]}}' --type=merge || true

          echo "ðŸ—‘ï¸ Deleting Ingresses and Applications..."
          kubectl delete ingress --all -A --timeout=60s || true
          kubectl delete applicationset.argoproj.io --all -n argocd --timeout=60s || true
          kubectl delete applications.argoproj.io --all -n argocd --timeout=60s || true

          echo "âŒ› Waiting for Kubernetes resources to fully delete..."
          # Wait for K8s resources (Ingress, Apps) to disappear after finalizers removal
          for i in {1..10}; do
            remaining_appsets=$(kubectl get applicationset.argoproj.io -n argocd --no-headers 2>/dev/null | wc -l)
            remaining_apps=$(kubectl get applications.argoproj.io -n argocd --no-headers 2>/dev/null | wc -l)
            remaining_ing=$(kubectl get ingress -A --no-headers 2>/dev/null | wc -l)

            if [ "$remaining_appsets" -eq 0 ] && [ "$remaining_apps" -eq 0 ] && [ "$remaining_ing" -eq 0 ]; then
              echo "âœ… All ArgoCD and Ingress resources deleted from Kubernetes."
              break
            fi

            echo "â³ Still removing... (AppSets: $remaining_appsets, Apps: $remaining_apps, Ingress: $remaining_ing)"
            sleep 10
          done
          
          # =========================================================
          # ðŸš¨ CRITICAL: Direct AWS CLI Cleanup (Forced Deletion)
          # This step ensures the ALB and Target Groups are deleted even if 
          # the AWS Load Balancer Controller failed to clean them up.
          # =========================================================
          echo "ðŸ”¨ Starting Direct AWS Load Balancer Cleanup..."

          # Discover ALB automatically by the Ingress Stack Tag
          ING_STACK="default/flask-app-ingress"

          ALB_ARN=$(aws elbv2 describe-load-balancers \
            --region "${{ env.AWS_REGION }}" \
            --query "LoadBalancers[?contains(Tags[?Key=='ingress.k8s.aws/stack'].Value | [0], \`${ING_STACK}\`)].LoadBalancerArn" \
            --output text --no-paginate --include-tags)

          if [ -z "$ALB_ARN" ] || [ "$ALB_ARN" = "None" ]; then
            echo "âœ… No ALB found for stack $ING_STACK"
          else
            echo "ðŸ—‘ï¸ Found ALB for stack $ING_STACK"
            echo "   ARN: $ALB_ARN"

            # Disable deletion protection
            aws elbv2 modify-load-balancer-attributes \
              --load-balancer-arn "$ALB_ARN" \
              --attributes Key=deletion_protection.enabled,Value=false \
              --region "${{ env.AWS_REGION }}" || true

            echo "ðŸ—‘ï¸ Deleting ALB..."
            aws elbv2 delete-load-balancer \
              --load-balancer-arn "$ALB_ARN" \
              --region "${{ env.AWS_REGION }}" || true

            # Wait for deletion
            for i in {1..20}; do
              CHECK=$(aws elbv2 describe-load-balancers \
                --load-balancer-arns "$ALB_ARN" \
                --region "${{ env.AWS_REGION }}" 2>&1 || echo "DELETED")

              if [[ "$CHECK" == "DELETED" ]]; then
                echo "âœ… ALB deleted."
                break
              fi
              echo "â³ Waiting for ALB to disappear..."
              sleep 5
            done
          fi

          # Cleanup Target Groups matching the same stack tag
          TG_ARNS=$(aws elbv2 describe-target-groups \
            --region "${{ env.AWS_REGION }}" \
            --query "TargetGroups[?contains(Tags[?Key=='ingress.k8s.aws/stack'].Value | [0], \`${ING_STACK}\`)].TargetGroupArn" \
            --output text --no-paginate --include-tags)

          if [ -n "$TG_ARNS" ]; then
            echo "ðŸ—‘ï¸ Deleting Target Groups related to $ING_STACK..."
            for TG in $TG_ARNS; do
              echo "   -> Deleting TG: $TG"
              aws elbv2 delete-target-group \
                --target-group-arn "$TG" \
                --region "${{ env.AWS_REGION }}" || true
            done
            echo "âœ… Target groups removed."
          else
            echo "âœ… No related TGs found."
          fi





  destroy-bootstrap:
    needs: deploy
    if: github.event.inputs.action == 'destroy' && success()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.7

      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: github-actions

      - name: Terraform Init
        run: |
          cd infra
          terraform init -upgrade

      - name: Destroy Bootstrap Infra
        run: |
          cd infra
          terraform destroy -auto-approve
