name: CD to EKS

on:
  workflow_run:
    workflows: ["CI Build & Test"]
    types:
      - completed

  workflow_dispatch:
    inputs:
      action:
        description: "Choose action"
        required: true
        default: "apply"
        type: choice
        options:
          - apply
          - destroy

permissions:
  id-token: write
  contents: read

env:
  aws_region: us-west-2
  cluster_name: eksdemo-cluster
  repo_name: eksdemo
  vpc_name: eksdemo-vpc
  domain_name: saharbittman.com
  argocd_clb_name: eksdemo-argocd-clb
  flask_alb_name: eksdemo-flask-alb

jobs:
  deploy:
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'workflow_run' &&
       github.event.workflow_run.conclusion == 'success')
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.aws_region }}
          role-session-name: github-actions

      - name: Check if EKS cluster exists
        id: check_cluster
        run: |
          if aws eks describe-cluster --name ${{ env.cluster_name }} --region ${{ env.aws_region }} 2>/dev/null; then
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
            echo "‚úÖ EKS cluster exists"
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è EKS cluster does not exist yet"
          fi

      - name: Wait for cluster to be ready
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        run: |
          echo "Waiting for EKS cluster to be in ACTIVE state..."
          for i in {1..30}; do
            STATUS=$(aws eks describe-cluster --name ${{ env.cluster_name }} --region ${{ env.aws_region }} --query 'cluster.status' --output text)
            if [ "$STATUS" = "ACTIVE" ]; then
              echo "‚úÖ Cluster is ACTIVE"
              break
            fi
            echo "‚è≥ Cluster status: $STATUS (attempt $i/30)"
            sleep 10
          done

      - name: Update kubeconfig for EKS
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        run: |
          aws eks update-kubeconfig --region ${{ env.aws_region }} --name ${{ env.cluster_name }}
          kubectl get nodes || echo "‚ö†Ô∏è No nodes ready yet"

      - name: Patch aws-auth
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        run: |
          cat <<'EOF' > aws-auth.yaml
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapUsers: |
              - userarn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_NUMBER }}:user/${{ secrets.AWS_USER }}
                username: ${{ secrets.AWS_USER }}
                groups:
                  - system:masters
          EOF
          kubectl apply -f aws-auth.yaml
          echo "‚úÖ aws-auth patched successfully"

      - name: Install / Upgrade ArgoCD Helm Chart
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        id: install_argocd
        run: |
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=LoadBalancer \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"="classic" \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-name"="${{ env.argocd_clb_name }}" \
            --set server.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-target-group-name"="eksdemo-argocd-tg"
          kubectl -n argocd rollout status deployment/argocd-server --timeout=5m
          echo "‚úÖ ArgoCD installed successfully"

      - name: Wait for ArgoCD LoadBalancer
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        id: argocd_lb
        run: |
          echo "Waiting for ArgoCD LoadBalancer..."
          for i in {1..30}; do
            ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [[ -n "$ARGOCD_SERVER" ]]; then
              echo "ARGOCD_SERVER=$ARGOCD_SERVER" >> $GITHUB_ENV
              echo "‚úÖ ArgoCD LoadBalancer ready: $ARGOCD_SERVER"
              break
            fi
            echo "‚è≥ Waiting for LoadBalancer (attempt $i/30)..."
            sleep 10
          done
          if [[ -z "$ARGOCD_SERVER" ]]; then
            echo "‚ùå Failed to get ArgoCD LoadBalancer"
            exit 1
          fi

      - name: Get VPC ID and set environment variables
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        id: get_vpc
        run: |
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${{ env.vpc_name }}" \
            --query "Vpcs[0].VpcId" \
            --output text)
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
          echo "ECR_NAME=${{ secrets.ECR_NAME }}" >> $GITHUB_ENV
          echo "AWS_ACCOUNT_NUMBER=${{ secrets.AWS_ACCOUNT_NUMBER }}" >> $GITHUB_ENV
          echo "‚úÖ VPC ID: $VPC_ID"

      - name: Apply ArgoCD Yamls
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        id: apply_yamls
        run: |
          echo "Applying ArgoCD configurations..."
          envsubst < ArgoCD/externalCharts/infra.yaml | kubectl apply -f -
          envsubst < ArgoCD/myChart/argo.yaml | kubectl apply -f -
          sleep 10
          kubectl get applications -n argocd
          echo "‚úÖ ArgoCD YAMLs applied successfully"
          
      - name: Smoke Test Application
        if: steps.check_cluster.outputs.cluster_exists == 'true' && github.event.inputs.action == 'apply'
        id: smoke_test
        run: |
          echo "Running smoke test on https://www.saharbittman.com ..."
          for i in $(seq 1 100); do
            STATUS_CODE=$(curl -s -o /dev/null -w "%{http_code}" -L https://www.saharbittman.com || true)
            if [ "$STATUS_CODE" = "200" ]; then
              echo "‚úÖ Smoke test passed."
              exit 0
            fi
            echo "‚ö†Ô∏è Attempt $i failed ‚Äî HTTP status: $STATUS_CODE"
            sleep 15
          done
          echo "‚ùå Smoke test failed!"
          exit 1

      - name: Cleanup on Failure
        if: |
          failure() &&
          github.event.inputs.action == 'apply' &&
          steps.check_cluster.outputs.cluster_exists == 'true'
        env:
          AWS_REGION: ${{ env.aws_region }}
        run: |
          echo "üßπ Starting cleanup due to deployment failure..."
          
          # Update kubeconfig
          aws eks update-kubeconfig --region $AWS_REGION --name ${{ env.cluster_name }} || true
          
          # Remove finalizers
          echo "Removing finalizers..."
          kubectl get applications.argoproj.io -n argocd -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl get applicationsets.argoproj.io -n argocd -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl get ingress -A -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -p '{"metadata":{"finalizers":[]}}' --type=merge || true

          # Delete ArgoCD resources
          echo "Deleting ArgoCD resources..."
          kubectl delete applications.argoproj.io --all -n argocd --timeout=60s || true
          kubectl delete applicationsets.argoproj.io --all -n argocd --timeout=60s || true
          kubectl delete ingress --all -A --timeout=60s || true
          helm uninstall argocd -n argocd --wait --timeout=2m || true
          kubectl delete svc -n argocd argocd-server --ignore-not-found || true

          # Wait for AWS to process the deletion
          echo "Waiting for resources to be cleaned up..."
          sleep 20

          # Delete Flask ALB
          echo "üîç Checking for Flask ALB: ${{ env.flask_alb_name }}..."
          FLASK_ALB_ARN=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" --names "${{ env.flask_alb_name }}" --query "LoadBalancers[0].LoadBalancerArn" --output text 2>/dev/null || echo "")
          
          if [[ -n "$FLASK_ALB_ARN" && "$FLASK_ALB_ARN" != "None" && "$FLASK_ALB_ARN" != "" ]]; then
            echo "Found Flask ALB, deleting Target Groups..."
            for TG in $(aws elbv2 describe-target-groups --load-balancer-arn "$FLASK_ALB_ARN" --region "$AWS_REGION" --query "TargetGroups[].TargetGroupArn" --output text 2>/dev/null || echo ""); do
              echo "Deregistering targets from TG: $TG"
              TARGETS=$(aws elbv2 describe-target-health --target-group-arn "$TG" --region "$AWS_REGION" --query "TargetHealthDescriptions[].Target.Id" --output text 2>/dev/null || echo "")
              for T in $TARGETS; do
                aws elbv2 deregister-targets --target-group-arn "$TG" --targets Id=$T --region "$AWS_REGION" || true
              done
              echo "Deleting Target Group: $TG"
              aws elbv2 delete-target-group --target-group-arn "$TG" --region "$AWS_REGION" || true
            done
            echo "Deleting Flask ALB: ${{ env.flask_alb_name }}"
            aws elbv2 delete-load-balancer --load-balancer-arn "$FLASK_ALB_ARN" --region "$AWS_REGION" || true
            echo "‚úÖ Flask ALB deleted"
          else
            echo "‚ÑπÔ∏è Flask ALB not found: ${{ env.flask_alb_name }}"
          fi

          # Delete ArgoCD Classic Load Balancer
          echo "üîç Checking for ArgoCD Classic Load Balancer: ${{ env.argocd_clb_name }}..."
          CLB_EXISTS=$(aws elb describe-load-balancers --region "$AWS_REGION" --load-balancer-names "${{ env.argocd_clb_name }}" --query "LoadBalancerDescriptions[0].LoadBalancerName" --output text 2>/dev/null || echo "")
          
          if [[ -n "$CLB_EXISTS" && "$CLB_EXISTS" != "None" && "$CLB_EXISTS" != "" ]]; then
            echo "Deleting ArgoCD Classic Load Balancer: ${{ env.argocd_clb_name }}"
            aws elb delete-load-balancer --load-balancer-name "${{ env.argocd_clb_name }}" --region "$AWS_REGION" || true
            echo "‚úÖ ArgoCD CLB deleted"
          else
            echo "‚ÑπÔ∏è ArgoCD CLB not found: ${{ env.argocd_clb_name }}"
          fi

          # Delete Target Groups by name pattern
          echo "üîç Checking for orphaned Target Groups..."
          TG_ARNS=$(aws elbv2 describe-target-groups --region "$AWS_REGION" --query "TargetGroups[?contains(TargetGroupName, 'eksdemo')].TargetGroupArn" --output text 2>/dev/null || echo "")
          
          if [[ -n "$TG_ARNS" && "$TG_ARNS" != "None" && "$TG_ARNS" != "" ]]; then
            for TG in $TG_ARNS; do
              echo "Deleting orphaned Target Group: $TG"
              aws elbv2 delete-target-group --target-group-arn "$TG" --region "$AWS_REGION" 2>/dev/null || true
            done
            echo "‚úÖ Orphaned Target Groups deleted"
          else
            echo "‚ÑπÔ∏è No orphaned Target Groups found"
          fi
          
          echo "‚úÖ Cleanup completed"

      - name: Destroy ArgoCD, Apps, and All Load Balancers
        if: github.event.inputs.action == 'destroy' && steps.check_cluster.outputs.cluster_exists == 'true'
        env:
          AWS_REGION: ${{ env.aws_region }}
        run: |
          echo "üóëÔ∏è Starting full destroy process..."
          
          # Update kubeconfig
          aws eks update-kubeconfig --region $AWS_REGION --name ${{ env.cluster_name }} || true
          
          # Remove finalizers
          echo "Removing finalizers..."
          kubectl get applications.argoproj.io -n argocd -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl get applicationsets.argoproj.io -n argocd -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -n argocd -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl get ingress -A -o name 2>/dev/null | \
            xargs -r -I {} kubectl patch {} -p '{"metadata":{"finalizers":[]}}' --type=merge || true

          # Delete all Kubernetes resources
          echo "Deleting all ArgoCD applications and ingresses..."
          kubectl delete applications.argoproj.io --all -n argocd --timeout=60s || true
          kubectl delete applicationsets.argoproj.io --all -n argocd --timeout=60s || true
          kubectl delete ingress --all -A --timeout=60s || true
          
          echo "Uninstalling ArgoCD..."
          helm uninstall argocd -n argocd --wait --timeout=2m || true
          kubectl delete namespace argocd --timeout=60s || true
          kubectl delete svc -n argocd argocd-server --ignore-not-found || true

          # Wait for AWS to process deletions
          echo "‚è≥ Waiting for Kubernetes resources to be cleaned up..."
          sleep 30

          # Delete Flask Application ALB
          echo "üóëÔ∏è Deleting Flask Application ALB: ${{ env.flask_alb_name }}..."
          FLASK_ALB_ARN=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" --names "${{ env.flask_alb_name }}" --query "LoadBalancers[0].LoadBalancerArn" --output text 2>/dev/null || echo "")
          
          if [[ -n "$FLASK_ALB_ARN" && "$FLASK_ALB_ARN" != "None" && "$FLASK_ALB_ARN" != "" ]]; then
            echo "Found Flask ALB: $FLASK_ALB_ARN"
            
            # Get and delete all listeners
            echo "Deleting ALB listeners..."
            LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$FLASK_ALB_ARN" --region "$AWS_REGION" --query "Listeners[].ListenerArn" --output text 2>/dev/null || echo "")
            for LISTENER in $LISTENERS; do
              aws elbv2 delete-listener --listener-arn "$LISTENER" --region "$AWS_REGION" || true
            done
            
            # Get and delete target groups
            echo "Deleting Flask ALB Target Groups..."
            for TG in $(aws elbv2 describe-target-groups --load-balancer-arn "$FLASK_ALB_ARN" --region "$AWS_REGION" --query "TargetGroups[].TargetGroupArn" --output text 2>/dev/null || echo ""); do
              echo "Processing Target Group: $TG"
              TARGETS=$(aws elbv2 describe-target-health --target-group-arn "$TG" --region "$AWS_REGION" --query "TargetHealthDescriptions[].Target.Id" --output text 2>/dev/null || echo "")
              for T in $TARGETS; do
                echo "Deregistering target: $T"
                aws elbv2 deregister-targets --target-group-arn "$TG" --targets Id=$T --region "$AWS_REGION" || true
              done
            done
            
            # Wait for deregistration
            sleep 10
            
            # Delete target groups
            for TG in $(aws elbv2 describe-target-groups --load-balancer-arn "$FLASK_ALB_ARN" --region "$AWS_REGION" --query "TargetGroups[].TargetGroupArn" --output text 2>/dev/null || echo ""); do
              echo "Deleting Target Group: $TG"
              aws elbv2 delete-target-group --target-group-arn "$TG" --region "$AWS_REGION" || true
            done
            
            # Delete the ALB
            echo "Deleting Flask ALB..."
            aws elbv2 delete-load-balancer --load-balancer-arn "$FLASK_ALB_ARN" --region "$AWS_REGION" || true
            echo "‚úÖ Flask ALB deleted"
          else
            echo "‚ÑπÔ∏è Flask ALB not found: ${{ env.flask_alb_name }}"
          fi

          # Delete ArgoCD Classic Load Balancer
          echo "üóëÔ∏è Deleting ArgoCD Classic Load Balancer: ${{ env.argocd_clb_name }}..."
          CLB_EXISTS=$(aws elb describe-load-balancers --region "$AWS_REGION" --load-balancer-names "${{ env.argocd_clb_name }}" --query "LoadBalancerDescriptions[0].LoadBalancerName" --output text 2>/dev/null || echo "")
          
          if [[ -n "$CLB_EXISTS" && "$CLB_EXISTS" != "None" && "$CLB_EXISTS" != "" ]]; then
            echo "Deleting ArgoCD Classic Load Balancer..."
            aws elb delete-load-balancer --load-balancer-name "${{ env.argocd_clb_name }}" --region "$AWS_REGION" || true
            echo "‚úÖ ArgoCD CLB deleted"
          else
            echo "‚ÑπÔ∏è ArgoCD CLB not found: ${{ env.argocd_clb_name }}"
          fi

          # Clean up any orphaned Target Groups
          echo "üßπ Cleaning up orphaned Target Groups..."
          TG_ARNS=$(aws elbv2 describe-target-groups --region "$AWS_REGION" --query "TargetGroups[?contains(TargetGroupName, 'eksdemo')].TargetGroupArn" --output text 2>/dev/null || echo "")
          
          if [[ -n "$TG_ARNS" && "$TG_ARNS" != "None" && "$TG_ARNS" != "" ]]; then
            for TG in $TG_ARNS; do
              echo "Attempting to delete orphaned Target Group: $TG"
              aws elbv2 delete-target-group --target-group-arn "$TG" --region "$AWS_REGION" 2>/dev/null || true
            done
            echo "‚úÖ Orphaned Target Groups cleaned up"
          else
            echo "‚ÑπÔ∏è No orphaned Target Groups found"
          fi

          # Clean up any remaining ALBs that match our naming pattern
          echo "üßπ Checking for any remaining ALBs with 'eksdemo' pattern..."
          REMAINING_ALBS=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" --query "LoadBalancers[?contains(LoadBalancerName, 'eksdemo')].LoadBalancerArn" --output text 2>/dev/null || echo "")
          
          if [[ -n "$REMAINING_ALBS" && "$REMAINING_ALBS" != "None" && "$REMAINING_ALBS" != "" ]]; then
            for ALB_ARN in $REMAINING_ALBS; do
              ALB_NAME=$(aws elbv2 describe-load-balancers --load-balancer-arns "$ALB_ARN" --region "$AWS_REGION" --query "LoadBalancers[0].LoadBalancerName" --output text)
              echo "Deleting remaining ALB: $ALB_NAME"
              aws elbv2 delete-load-balancer --load-balancer-arn "$ALB_ARN" --region "$AWS_REGION" || true
            done
            echo "‚úÖ Remaining ALBs deleted"
          else
            echo "‚ÑπÔ∏è No remaining ALBs found"
          fi
          
          echo "‚úÖ Full destroy completed successfully!"

      - name: Skip deployment - Cluster not ready
        if: steps.check_cluster.outputs.cluster_exists == 'false' && github.event.inputs.action == 'apply'
        run: |
          echo "‚ö†Ô∏è EKS cluster '${{ env.cluster_name }}' does not exist or is not ready yet."
          echo "Please run the infrastructure workflow first to create the cluster."
          echo "This workflow will run automatically once the cluster is ready."
          exit 0